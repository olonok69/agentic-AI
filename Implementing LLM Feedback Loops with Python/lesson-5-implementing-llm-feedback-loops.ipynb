{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lesson 5: Implementing LLM Feedback Loops\n",
        "\n",
        "## Code Generation and Debugging Assistant\n",
        "\n",
        "In this hands-on exercise, you will implement iterative feedback loops where an AI generates, tests, and revises Python code snippets based on test results and feedback.\n",
        "\n",
        "We will use an LLM Feedback loop to create and iteratively improve a Python function called `process_data` that is best described using the following examples:\n",
        "\n",
        "```python\n",
        "process_data([1, 2, 3, 4, 5], mode='average')  # Should return 3.0\n",
        "process_data([1, 2, 'a', 3], mode='sum')  # Should return 6\n",
        "```\n",
        "\n",
        "\n",
        "### Outline:\n",
        "\n",
        "- Setup\n",
        "- Define Task and Test Cases\n",
        "- Initial Generation\n",
        "- Expand the Test Cases\n",
        "- First Iteration with Feedback\n",
        "- Create Feedback Loop\n",
        "- Reflection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup\n",
        "\n",
        "Import necessary libraries and define helper functions, including a mock LLM client, code execution environment, and test runner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "# No changes needed in this cell\n",
        "from openai import OpenAI\n",
        "from IPython.display import Markdown, display\n",
        "import traceback\n",
        "import io\n",
        "import os\n",
        "from contextlib import redirect_stdout, redirect_stderr\n",
        "from enum import Enum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up LLM credentials\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=\"https://openai.vocareum.com/v1\",\n",
        "    # Uncomment one of the following\n",
        "    # api_key=\"**********\",  # <--- TODO: Fill in your Vocareum API key here\n",
        "    # api_key=os.getenv(\n",
        "    #     \"OPENAI_API_KEY\"\n",
        "    # ),  # <-- Alternately, set as an environment variable\n",
        ")\n",
        "\n",
        "# If using OpenAI's API endpoint\n",
        "# client = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define helper functions\n",
        "# No changes needed in this cell\n",
        "\n",
        "\n",
        "class OpenAIModels(str, Enum):\n",
        "    GPT_4O_MINI = \"gpt-4o-mini\"\n",
        "    GPT_41_MINI = \"gpt-4.1-mini\"\n",
        "    GPT_41_NANO = \"gpt-4.1-nano\"\n",
        "\n",
        "\n",
        "MODEL = OpenAIModels.GPT_41_NANO\n",
        "\n",
        "\n",
        "def get_completion(messages=None, system_prompt=None, user_prompt=None, model=MODEL):\n",
        "    \"\"\"\n",
        "    Function to get a completion from the OpenAI API.\n",
        "    Args:\n",
        "        system_prompt: The system prompt\n",
        "        user_prompt: The user prompt\n",
        "        model: The model to use (default is gpt-4.1-mini)\n",
        "    Returns:\n",
        "        The completion text\n",
        "    \"\"\"\n",
        "\n",
        "    messages = list(messages)\n",
        "    if system_prompt:\n",
        "        messages.insert(0, {\"role\": \"system\", \"content\": system_prompt})\n",
        "    if user_prompt:\n",
        "        messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0.7,\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "def execute_code(code, test_cases):\n",
        "    \"\"\"\n",
        "    Executes Python code and returns the results of test cases.\n",
        "    Args:\n",
        "        code: String containing Python code\n",
        "        test_cases: List of dictionaries with inputs and expected outputs\n",
        "    Returns:\n",
        "        Dictionary containing execution results and test outcomes\n",
        "    \"\"\"\n",
        "    results = {\"execution_error\": None, \"test_results\": [], \"passed\": 0, \"failed\": 0}\n",
        "\n",
        "    # Create a namespace for execution\n",
        "    namespace = {}\n",
        "\n",
        "    # Capture stdout and stderr\n",
        "    output_buffer = io.StringIO()\n",
        "\n",
        "    try:\n",
        "        with redirect_stdout(output_buffer), redirect_stderr(output_buffer):\n",
        "            exec(code, namespace)\n",
        "\n",
        "        # Run test cases\n",
        "        for i, test in enumerate(test_cases):\n",
        "            inputs = test[\"inputs\"]\n",
        "            expected = test[\"expected\"]\n",
        "\n",
        "            # Execute the function with test inputs\n",
        "            try:\n",
        "                if isinstance(inputs, dict):\n",
        "                    actual = namespace[\"process_data\"](**inputs)\n",
        "                else:\n",
        "                    actual = namespace[\"process_data\"](*inputs)\n",
        "\n",
        "                passed = actual == expected\n",
        "\n",
        "                if passed:\n",
        "                    results[\"passed\"] += 1\n",
        "                else:\n",
        "                    results[\"failed\"] += 1\n",
        "\n",
        "                results[\"test_results\"].append(\n",
        "                    {\n",
        "                        \"test_id\": i + 1,\n",
        "                        \"inputs\": inputs,\n",
        "                        \"expected\": expected,\n",
        "                        \"actual\": actual,\n",
        "                        \"passed\": passed,\n",
        "                    }\n",
        "                )\n",
        "            except Exception as e:\n",
        "                # If the error is the expected type, mark as passed\n",
        "                passed = isinstance(expected, type) and isinstance(e, expected)\n",
        "                results[\"test_results\"].append(\n",
        "                    {\n",
        "                        \"test_id\": i + 1,\n",
        "                        \"inputs\": inputs,\n",
        "                        \"expected\": expected,\n",
        "                        \"error\": str(e),\n",
        "                        \"passed\": passed,\n",
        "                    }\n",
        "                )\n",
        "                if passed:\n",
        "                    results[\"passed\"] += 1\n",
        "                else:\n",
        "                    results[\"failed\"] += 1\n",
        "\n",
        "    except Exception as e:\n",
        "        results[\"execution_error\"] = {\n",
        "            \"error_type\": type(e).__name__,\n",
        "            \"error_message\": str(e),\n",
        "            \"traceback\": traceback.format_exc(),\n",
        "        }\n",
        "\n",
        "    results[\"stdout\"] = output_buffer.getvalue()\n",
        "    return results\n",
        "\n",
        "\n",
        "# Function to format test results as feedback for the model\n",
        "def format_feedback(results):\n",
        "    \"\"\"\n",
        "    Formats test results into a clear feedback string for the model.\n",
        "    Args:\n",
        "        results: Dictionary containing execution results\n",
        "    Returns:\n",
        "        Formatted feedback string\n",
        "    \"\"\"\n",
        "    feedback = []\n",
        "\n",
        "    if results[\"execution_error\"]:\n",
        "        feedback.append(\n",
        "            f\"ERROR: Code execution failed with {results['execution_error']['error_type']}\"\n",
        "        )\n",
        "        feedback.append(f\"Message: {results['execution_error']['error_message']}\")\n",
        "        feedback.append(\"Traceback:\")\n",
        "        feedback.append(results[\"execution_error\"][\"traceback\"])\n",
        "        feedback.append(\"\\nPlease fix the syntax or runtime errors in the code.\")\n",
        "        return \"\\n\".join(feedback)\n",
        "\n",
        "    feedback.append(\n",
        "        f\"Test Results: {results['passed']} passed, {results['failed']} failed\"\n",
        "    )\n",
        "\n",
        "    if results[\"stdout\"]:\n",
        "        feedback.append(f\"\\nStandard output:\\n{results['stdout']}\")\n",
        "\n",
        "    if results[\"failed\"] > 0:\n",
        "        feedback.append(\"\\nFailed Test Cases:\")\n",
        "        for test in results[\"test_results\"]:\n",
        "            if not test.get(\"passed\"):\n",
        "                feedback.append(f\"\\nTest #{test['test_id']}:\")\n",
        "                feedback.append(f\"  Inputs: {test['inputs']}\")\n",
        "                feedback.append(f\"  Expected: {test['expected']}\")\n",
        "                if \"actual\" in test:\n",
        "                    feedback.append(f\"  Actual: {test['actual']}\")\n",
        "                if \"error\" in test:\n",
        "                    feedback.append(f\"  Error: {test['error']}\")\n",
        "\n",
        "    return \"\\n\".join(feedback)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Define Task and Test Cases\n",
        "\n",
        "We will create a Python function called `process_data` that analyzes numerical data with the following (possibly incomplete) set of requirements:\n",
        "\n",
        "1. The function should accept a list of numbers and an optional parameter 'mode' that can be 'sum' or 'average' (default should be 'average').\n",
        "2. If mode is 'sum', return the sum of all numbers.\n",
        "3. If mode is 'average', return the average (mean) of all numbers.\n",
        "\n",
        "Example:\n",
        "```python\n",
        "process_data([1, 2, 3, 4, 5], mode='average')  # Should return 3.0\n",
        "process_data([1, 2, 'a', 3], mode='sum')  # Should return 6\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write out a task description for the LLM\n",
        "# TODO: Complete this cell by replacing the **********\n",
        "task_description = \"\"\"\n",
        "We will create a Python function called `process_data` that analyzes numerical data with the following (possibly incomplete) set of requirements:\n",
        "\n",
        "1. The function should accept a list of numbers and an optional parameter 'mode' that can be 'sum' or 'average' (default should be 'average').\n",
        "2. If mode is 'sum', return the sum of all numbers.\n",
        "3. If mode is 'average', return the average (mean) of all numbers.\n",
        "\n",
        "Example:\n",
        "```python\n",
        "process_data([1, 2, 3, 4, 5], mode='average')  # Should return 3.0\n",
        "process_data([1, 2, 'a', 3], mode='sum')  # Should return 6\n",
        "```\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write out test cases to test the LLM's output\n",
        "# TODO: Add more test cases following the examples provided replacing the **********\n",
        "test_cases = [\n",
        "    {\"inputs\": ([1, 2, 3, 4, 5], \"sum\"), \"expected\": 15},\n",
        "    {\"inputs\": ([1, 2, 3, 4, 5], \"average\"), \"expected\": 3.0},\n",
        "    {\"inputs\": ([11, 12, 13, 14, 15], \"sum\"), \"expected\": 65},\n",
        "    {\"inputs\": ([11, 12, 13, 14, 15], \"average\"), \"expected\": 13.0},\n",
        "    {\"inputs\": ([1.1, 2.2, 3.3, 4.4, 5.5], \"sum\"), \"expected\": 16.5},\n",
        "    {\"inputs\": ([1.1, 2.2, 3.3, 4.4, 5.5], \"average\"), \"expected\": 3.3},\n",
        "    {\"inputs\": ([-1, -2, -3, -4, -5], \"sum\"), \"expected\": -15},\n",
        "    {\"inputs\": ([-1, -2, -3, -4, -5], \"average\"), \"expected\": -3.0},\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Initial Generation\n",
        "\n",
        "Let's start with a basic prompt to generate an initial solution to our problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Generated Code:\n",
            "def process_data(numbers, mode='average'):\n",
            "    # Filter out non-numeric values\n",
            "    numeric_values = [num for num in numbers if isinstance(num, (int, float))]\n",
            "\n",
            "    if not numeric_values:\n",
            "        return None  # or raise an error if preferred\n",
            "\n",
            "    if mode == 'sum':\n",
            "        return sum(numeric_values)\n",
            "    elif mode == 'average':\n",
            "        return sum(numeric_values) / len(numeric_values)\n",
            "    else:\n",
            "        # Unsupported mode\n",
            "        return None\n",
            "\n",
            "Test Results:\n",
            "Test Results: 8 passed, 0 failed\n"
          ]
        }
      ],
      "source": [
        "# Basic prompt for initial code generation\n",
        "# TODO: Complete this cell by replacing the **********\n",
        "initial_prompt = f\"\"\"\n",
        "You are an expert Python developer. Please write a Python function based on the following requirements:\n",
        "\n",
        "{task_description}\n",
        "\n",
        "Write only the function surrounded by ```python and ``` without any additional explanations or examples.\n",
        "\n",
        "Example:\n",
        "\n",
        "```python\n",
        "def function_name(arguments):\n",
        "    # function body\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "# Get initial completion\n",
        "messages = [{\"role\": \"user\", \"content\": initial_prompt}]\n",
        "initial_response = get_completion(messages)\n",
        "\n",
        "\n",
        "def extract_code(code):\n",
        "    lines = code.split(\"\\n\")\n",
        "    start = lines.index(\"```python\") + 1\n",
        "    end = lines.index(\"```\", start)\n",
        "    return \"\\n\".join(lines[start:end])\n",
        "\n",
        "\n",
        "initial_code = extract_code(initial_response)\n",
        "\n",
        "print(\"Initial Generated Code:\")\n",
        "print(initial_code)\n",
        "\n",
        "# Execute and test the initial code\n",
        "initial_results = execute_code(initial_code, test_cases)\n",
        "initial_feedback = format_feedback(initial_results)\n",
        "\n",
        "print(\"\\nTest Results:\")\n",
        "print(initial_feedback)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Expand the Test Cases\n",
        "\n",
        "Now, pretend that you've used this code in a production setting and have received feedback. The first version of your generated code worked marvelously, and now you are seeking to expand the capabilities of your function.\n",
        "\n",
        "Unfortunately, your product manager is on vacation, but you have know your function needs to:\n",
        "1) support a new mode, \"median\"\n",
        "2) ignore non-numeric values\n",
        "3) handle empty lists, returning None\n",
        "\n",
        "So, following test-driven development practices, you update your tests:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# These are the new test cases. No updates needed.\n",
        "test_cases = [\n",
        "    {\"inputs\": ([1, 2, 3, 4, 5], \"sum\"), \"expected\": 15},\n",
        "    {\"inputs\": ([1, 2, 3, 4, 5], \"average\"), \"expected\": 3.0},\n",
        "    {\"inputs\": ([11, 12, 13, 14, 15], \"sum\"), \"expected\": 65},\n",
        "    {\"inputs\": ([11, 12, 13, 14, 15], \"average\"), \"expected\": 13.0},\n",
        "    {\"inputs\": ([], \"sum\"), \"expected\": None},\n",
        "    {\"inputs\": ([1, 3, 4], \"median\"), \"expected\": 3},\n",
        "    {\"inputs\": ([1, 2, 3, 5], \"median\"), \"expected\": 2.5},\n",
        "    {\"inputs\": ([1, 2, \"a\", 3], \"sum\"), \"expected\": 6},\n",
        "    {\"inputs\": ([1, 2, None, 3, \"b\", 4], \"average\"), \"expected\": 2.5},\n",
        "    {\"inputs\": ([10], \"median\"), \"expected\": 10},\n",
        "    {\"inputs\": ([], \"median\"), \"expected\": None},\n",
        "    {\"inputs\": ([1, 2, 3, 4, 5], \"invalid_mode\"), \"expected\": ValueError},\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Generated Code:\n",
            "def process_data(numbers, mode='average'):\n",
            "    # Filter out non-numeric values\n",
            "    numeric_values = [num for num in numbers if isinstance(num, (int, float))]\n",
            "\n",
            "    if not numeric_values:\n",
            "        return None  # or raise an error if preferred\n",
            "\n",
            "    if mode == 'sum':\n",
            "        return sum(numeric_values)\n",
            "    elif mode == 'average':\n",
            "        return sum(numeric_values) / len(numeric_values)\n",
            "    else:\n",
            "        # Unsupported mode\n",
            "        return None\n",
            "\n",
            "Test Results:\n",
            "Test Results: 8 passed, 4 failed\n",
            "\n",
            "Failed Test Cases:\n",
            "\n",
            "Test #6:\n",
            "  Inputs: ([1, 3, 4], 'median')\n",
            "  Expected: 3\n",
            "  Actual: None\n",
            "\n",
            "Test #7:\n",
            "  Inputs: ([1, 2, 3, 5], 'median')\n",
            "  Expected: 2.5\n",
            "  Actual: None\n",
            "\n",
            "Test #10:\n",
            "  Inputs: ([10], 'median')\n",
            "  Expected: 10\n",
            "  Actual: None\n",
            "\n",
            "Test #12:\n",
            "  Inputs: ([1, 2, 3, 4, 5], 'invalid_mode')\n",
            "  Expected: <class 'ValueError'>\n",
            "  Actual: None\n"
          ]
        }
      ],
      "source": [
        "# Re-test the code\n",
        "# No updates are needed in this cell\n",
        "print(\"Initial Generated Code:\")\n",
        "print(initial_code)\n",
        "\n",
        "# Execute and test the initial code\n",
        "initial_results = execute_code(initial_code, test_cases)\n",
        "initial_feedback = format_feedback(initial_results)\n",
        "\n",
        "print(\"\\nTest Results:\")\n",
        "print(initial_feedback)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. First Iteration with Feedback\n",
        "Now, let's feed the test results back to the model and ask for an improved version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Improved Code:\n",
            "def process_data(numbers, mode='average'):\n",
            "    # Filter out non-numeric values\n",
            "    numeric_values = [num for num in numbers if isinstance(num, (int, float))]\n",
            "\n",
            "    if not numeric_values:\n",
            "        return None  # or raise an error if preferred\n",
            "\n",
            "    if mode == 'sum':\n",
            "        return sum(numeric_values)\n",
            "    elif mode == 'average':\n",
            "        return sum(numeric_values) / len(numeric_values)\n",
            "    elif mode == 'median':\n",
            "        sorted_nums = sorted(numeric_values)\n",
            "        n = len(sorted_nums)\n",
            "        mid = n // 2\n",
            "        if n % 2 == 0:\n",
            "            return (sorted_nums[mid - 1] + sorted_nums[mid]) / 2\n",
            "        else:\n",
            "            return sorted_nums[mid]\n",
            "    else:\n",
            "        raise ValueError(f\"Invalid mode: {mode}\")\n",
            "\n",
            "Test Results for Improved Code:\n",
            "Test Results: 12 passed, 0 failed\n"
          ]
        }
      ],
      "source": [
        "# Prompt with feedback for first iteration\n",
        "# TODO: Complete this cell by replacing the **********\n",
        "feedback_prompt = f\"\"\"\n",
        "You are an expert Python developer. You wrote a function based on these requirements:\n",
        "\n",
        "{task_description}\n",
        "\n",
        "Here is your current implementation:\n",
        "```python\n",
        "{initial_code}\n",
        "```\n",
        "I've tested your code and here are the results:\n",
        "{initial_feedback}\n",
        "Please improve your code to fix any issues and make sure it passes all test cases.\n",
        "Write only the improved function without any explanation.\n",
        "\"\"\"\n",
        "\n",
        "messages = [{\"role\": \"user\", \"content\": feedback_prompt}]\n",
        "\n",
        "# Get improved code\n",
        "improved_response = get_completion(messages)\n",
        "\n",
        "# Extract the improved code\n",
        "improved_code = extract_code(improved_response)\n",
        "\n",
        "print(\"\\nImproved Code:\")\n",
        "print(improved_code)\n",
        "\n",
        "# Execute and test the improved code\n",
        "improved_results = execute_code(improved_code, test_cases)\n",
        "improved_feedback = format_feedback(improved_results)\n",
        "print(\"\\nTest Results for Improved Code:\")\n",
        "print(improved_feedback)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Create Feedback Loop\n",
        "\n",
        "We may want to give the LLM more than one chance to generate the correct code. We may even want to introduce test cases gradually, so that it has the opportunity to fix errors one at a time.\n",
        "\n",
        "Let's develop a loop that will start from scratch and run the loop a maximum number of times or until the code is correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'failed': 5, 'passed': 7}\n",
            "{'failed': 4, 'passed': 8}\n",
            "{'failed': 0, 'passed': 12}\n",
            "\n",
            "Success! All tests passed.\n"
          ]
        }
      ],
      "source": [
        "# Write a code-creation LLM loop\n",
        "# TODO: Replace the parts marked with *****\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "iterations = []\n",
        "\n",
        "\n",
        "# Get initial completion and extract code\n",
        "messages = [{\"role\": \"user\", \"content\": initial_prompt}]\n",
        "initial_response = get_completion(messages)\n",
        "initial_code = extract_code(initial_response)\n",
        "\n",
        "# Execute and test the initial code\n",
        "initial_results = execute_code(initial_code, test_cases)\n",
        "initial_feedback = format_feedback(initial_results)\n",
        "\n",
        "\n",
        "# Store the initial iteration\n",
        "iterations.append(\n",
        "    {\n",
        "        \"iteration\": 0,\n",
        "        \"code\": initial_code,\n",
        "        \"test_results\": {\n",
        "            \"passed\": initial_results[\"passed\"],\n",
        "            \"failed\": initial_results[\"failed\"],\n",
        "        },\n",
        "    }\n",
        ")\n",
        "\n",
        "pprint(iterations[-1][\"test_results\"])\n",
        "\n",
        "current_code = initial_code\n",
        "current_feedback = initial_feedback\n",
        "\n",
        "# Loop to improve the code based on feedback\n",
        "for i in range(3):\n",
        "    if iterations[-1][\"test_results\"][\"failed\"] == 0:\n",
        "        print(\"\\nSuccess! All tests passed.\")\n",
        "        break\n",
        "    feedback_prompt = f\"\"\"\n",
        "    You are an expert Python developer. You wrote a function based on these requirements:\n",
        "\n",
        "    {task_description}\n",
        "\n",
        "    Here is your current implementation:\n",
        "    ```python\n",
        "    {current_code}\n",
        "    ```\n",
        "    I've tested your code and here are the results:\n",
        "    {current_feedback}\n",
        "    Please improve your code to fix any issues and make sure it passes all test cases.\n",
        "    Write only the improved function without any explanation.\n",
        "    \"\"\"    \n",
        "\n",
        "    # Feedback prompt with retry\n",
        "    # feedback_prompt = ...\n",
        "    messages = [{\"role\": \"user\", \"content\": feedback_prompt}]\n",
        "    improved_response = get_completion(messages)\n",
        "    improved_code = extract_code(improved_response)\n",
        "\n",
        "    # Execute and test the improved code\n",
        "    improved_results = execute_code(improved_code, test_cases)\n",
        "    improved_feedback = format_feedback(improved_results)\n",
        "    iterations.append(\n",
        "        {\n",
        "            \"iteration\": i + 1,\n",
        "            \"code\": improved_code,\n",
        "            \"test_results\": {\n",
        "                \"passed\": improved_results[\"passed\"],\n",
        "                \"failed\": improved_results[\"failed\"],\n",
        "            },\n",
        "        }\n",
        "    )\n",
        "    pprint(iterations[-1][\"test_results\"])\n",
        "\n",
        "    current_code = improved_code\n",
        "    current_feedback = improved_feedback\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'code': \"def process_data(numbers, mode='average'):\\n\"\n",
            "          '    # Filter out non-numeric values\\n'\n",
            "          '    numeric_values = [num for num in numbers if isinstance(num, (int, float))]\\n'\n",
            "          '    if not numeric_values:\\n'\n",
            "          '        return 0\\n'\n",
            "          \"    if mode == 'sum':\\n\"\n",
            "          '        return sum(numeric_values)\\n'\n",
            "          \"    elif mode == 'average':\\n\"\n",
            "          '        return sum(numeric_values) / len(numeric_values)\\n'\n",
            "          '    else:\\n'\n",
            "          '        raise ValueError(\"Invalid mode. Choose \\'sum\\' or \\'average\\'.\")',\n",
            "  'iteration': 0,\n",
            "  'test_results': {'failed': 5, 'passed': 7}},\n",
            " {'code': \"def process_data(numbers, mode='average'):\\n\"\n",
            "          '    # Filter out non-numeric values\\n'\n",
            "          '    numeric_values = [num for num in numbers if isinstance(num, (int, float))]\\n'\n",
            "          '    if not numeric_values:\\n'\n",
            "          '        return None\\n'\n",
            "          \"    if mode == 'sum':\\n\"\n",
            "          '        return sum(numeric_values)\\n'\n",
            "          \"    elif mode == 'average':\\n\"\n",
            "          '        return sum(numeric_values) / len(numeric_values)\\n'\n",
            "          '    else:\\n'\n",
            "          '        return None',\n",
            "  'iteration': 1,\n",
            "  'test_results': {'failed': 4, 'passed': 8}},\n",
            " {'code': \"def process_data(numbers, mode='average'):\\n\"\n",
            "          '    # Filter out non-numeric values\\n'\n",
            "          '    numeric_values = [num for num in numbers if isinstance(num, (int, float))]\\n'\n",
            "          '    if not numeric_values:\\n'\n",
            "          '        return None\\n'\n",
            "          '\\n'\n",
            "          '    mode_lower = mode.lower()\\n'\n",
            "          '\\n'\n",
            "          \"    if mode_lower == 'sum':\\n\"\n",
            "          '        return sum(numeric_values)\\n'\n",
            "          \"    elif mode_lower == 'average':\\n\"\n",
            "          '        return sum(numeric_values) / len(numeric_values)\\n'\n",
            "          \"    elif mode_lower == 'median':\\n\"\n",
            "          '        sorted_values = sorted(numeric_values)\\n'\n",
            "          '        n = len(sorted_values)\\n'\n",
            "          '        mid = n // 2\\n'\n",
            "          '        if n % 2 == 1:\\n'\n",
            "          '            return sorted_values[mid]\\n'\n",
            "          '        else:\\n'\n",
            "          '            return (sorted_values[mid - 1] + sorted_values[mid]) / 2\\n'\n",
            "          '    else:\\n'\n",
            "          '        raise ValueError(\"Invalid mode. Choose \\'sum\\', \\'average\\', or \\'median\\'.\")',\n",
            "  'iteration': 2,\n",
            "  'test_results': {'failed': 0, 'passed': 12}}]\n"
          ]
        }
      ],
      "source": [
        "# View a summary of the different iterations\n",
        "from pprint import pprint\n",
        "pprint(iterations, width=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "def process_data(numbers, mode='average'):\n",
            "    # Filter out non-numeric values\n",
            "    numeric_values = [num for num in numbers if isinstance(num, (int, float))]\n",
            "    if not numeric_values:\n",
            "        return None\n",
            "\n",
            "    mode_lower = mode.lower()\n",
            "\n",
            "    if mode_lower == 'sum':\n",
            "        return sum(numeric_values)\n",
            "    elif mode_lower == 'average':\n",
            "        return sum(numeric_values) / len(numeric_values)\n",
            "    elif mode_lower == 'median':\n",
            "        sorted_values = sorted(numeric_values)\n",
            "        n = len(sorted_values)\n",
            "        mid = n // 2\n",
            "        if n % 2 == 1:\n",
            "            return sorted_values[mid]\n",
            "        else:\n",
            "            return (sorted_values[mid - 1] + sorted_values[mid]) / 2\n",
            "    else:\n",
            "        raise ValueError(\"Invalid mode. Choose 'sum', 'average', or 'median'.\")\n"
          ]
        }
      ],
      "source": [
        "# Print the final code\n",
        "print(iterations[-1][\"code\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Reflection & Transfer\n",
        "\n",
        "What improvements did you observe across iterations?\n",
        "Analyze how the code improved with each feedback loop iteration:\n",
        "\n",
        "* Correctness: Did the number of passed tests increase?\n",
        "* Error handling: How did error handling evolve?\n",
        "* Edge cases: How did handling of edge cases improve?\n",
        "* Readability: Did the code become more readable or maintainable?\n",
        "\n",
        "How effective was the feedback loop approach?\n",
        "Reflect on the effectiveness of using a feedback loop for code generation:\n",
        "\n",
        "* What types of issues were fixed in each iteration?\n",
        "* Were there any problems that persisted across iterations?\n",
        "* How could the feedback mechanism be improved?\n",
        "* How does this compare to traditional approaches to debugging and code refinement?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this exercise, we explored how LLM feedback loops can be used to iteratively improve code generation.\n",
        "\n",
        "By providing structured feedback about test failures, we enabled the model to focus on specific issues and incrementally improve its solution.\n",
        "\n",
        "The key insight is that well-structured feedback loops can significantly enhance the quality and correctness of AI-generated code, especially for complex tasks with multiple (possibly incomplete) requirements and edge cases.\n",
        "\n",
        "\n",
        "Congratulations on completing this exercise! Give yourself a hand! 🤗🤗"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
