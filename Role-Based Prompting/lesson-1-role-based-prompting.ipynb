{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 1: Role-Based Prompting (Agent Personas)\n",
    "\n",
    "## Historical Figure Interviewer\n",
    "\n",
    "In this hands-on exercise, you will craft prompts to instruct an AI to convincingly adopt the persona of a historical figure during an interactive Q&A session.\n",
    "\n",
    "## **Outline:**\n",
    "- Plain Prompt: Try a basic prompt without any specific role defined\n",
    "- Baseline Historical Figure Prompt: Ask the AI to role-play as a historical figure\n",
    "- Define Persona-Specific Attributes: Define personality, etc.\n",
    "- Add Tone and Style Specifications: Specify tone and style for the responses\n",
    "- Q&A Session Format: Conduct mock Q&A sessions to test persona consistency\n",
    "- Reflection & Transfer: Evaluate realism of AI responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using the Vocareum API endpoint\n",
    "\n",
    "\n",
    "# client = OpenAI(\n",
    "#     base_url=\"https://openai.vocareum.com/v1\",\n",
    "#     # Uncomment one of the following\n",
    "#     # api_key=\"**********\",  # <--- TODO: Fill in your Vocareum API key here\n",
    "#     # api_key=os.getenv(\n",
    "#     #     \"OPENAI_API_KEY\"\n",
    "#     # ),  # <-- Alternately, set as an environment variable\n",
    "# )\n",
    "\n",
    "load_dotenv()\n",
    "# If using OpenAI's API endpoint\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to display responses as Markdown, horizontally\n",
    "def display_responses(*args):\n",
    "    markdown_string = \"<table><tr>\"\n",
    "    # Headers\n",
    "    for arg in args:\n",
    "        markdown_string += f\"<th>System Prompt:<br />{arg['system_prompt']}<br /><br />\"\n",
    "        markdown_string += f\"User Prompt:<br />{arg['user_prompt']}</th>\"\n",
    "    markdown_string += \"</tr>\"\n",
    "    # Rows\n",
    "    markdown_string += \"<tr>\"\n",
    "    for arg in args:\n",
    "        markdown_string += f\"<td>Response:<br />{arg['response']}</td>\"\n",
    "    markdown_string += \"</tr></table>\"\n",
    "    display(Markdown(markdown_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to interact with the OpenAI API\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class OpenAIModels(str, Enum):\n",
    "    GPT_4O_MINI = \"gpt-4o-mini\"\n",
    "    GPT_41_MINI = \"gpt-4.1-mini\"\n",
    "    GPT_41_NANO = \"gpt-4.1-nano\"\n",
    "\n",
    "\n",
    "MODEL = OpenAIModels.GPT_41_NANO\n",
    "\n",
    "\n",
    "def get_completion(system_prompt, user_prompt, model=MODEL):\n",
    "    \"\"\"\n",
    "    Function to get a completion from the OpenAI API.\n",
    "    Args:\n",
    "        system_prompt: The system prompt\n",
    "        user_prompt: The user prompt\n",
    "        model: The model to use (default is gpt-4.1-mini)\n",
    "    Returns:\n",
    "        The completion text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Plain Prompt\n",
    "\n",
    "First, let's see what the model produces with a basic prompt before asking it to role-play as a historical figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_system_prompt = \"You are a helpful assistant.\"\n",
    "user_prompt = \"Can you tell me about relativity?\"\n",
    "\n",
    "print(f\"Sending prompt to {MODEL} model...\")\n",
    "control_response = get_completion(control_system_prompt, user_prompt)\n",
    "print(\"Response received!\\n\")\n",
    "\n",
    "display_responses(\n",
    "    {\n",
    "        \"system_prompt\": control_system_prompt,\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"response\": control_response,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline Historical Figure Prompt\n",
    "\n",
    "First, let's see what the model produces with a basic prompt asking it to role-play as Albert Einstein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add a role-based prompt for portraying Albert Einstein where you see the **********\n",
    "baseline_system_prompt = \"You are a Researcher from University and provide clear concept and explanations for an academia Audience\"\n",
    "user_prompt = \"Can you tell me about relativity?\"\n",
    "\n",
    "print(f\"Sending prompt to {MODEL} model...\")\n",
    "baseline_response = get_completion(baseline_system_prompt, user_prompt)\n",
    "print(\"Response received!\\n\")\n",
    "\n",
    "display_responses(\n",
    "    {\n",
    "        \"system_prompt\": baseline_system_prompt,\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"response\": baseline_response,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Persona-Specific Attributes (3 min)\n",
    "\n",
    "Now, let's enhance the prompt by adding specific attributes about Einstein's personality, era-specific vocabulary, and expertise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add persona-specific attributes where you see **********\n",
    "persona_system_prompt = f\"\"\"{baseline_system_prompt}.\n",
    "\n",
    "Adopt these persona characteristics:\n",
    "\n",
    "- Personality: Researcher, Academic background in Physics. \n",
    "- Speech style: Academic\n",
    "- Expertise: Physics, Maths\n",
    "- Historical context: Second Half of 20th Century, when first time the humanity got the moon\n",
    "  \n",
    "Answer as if you are Einstein speaking in 1950, reflecting on your life and work.\n",
    "Only discuss information that would have been known to you in your lifetime.\"\"\"\n",
    "\n",
    "user_prompt = \"Can you tell me about relativity?\"\n",
    "\n",
    "print(f\"Sending prompt to {MODEL} model...\")\n",
    "persona_response = get_completion(persona_system_prompt, user_prompt)\n",
    "print(\"Response received!\\n\")\n",
    "\n",
    "# Show last two prompts and responses\n",
    "display_responses(\n",
    "    {\n",
    "        \"system_prompt\": baseline_system_prompt,\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"response\": baseline_response,\n",
    "    },\n",
    "    {\n",
    "        \"system_prompt\": persona_system_prompt,\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"response\": persona_response,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Add Tone and Style Specifications (3 min)\n",
    "\n",
    "Let's further refine the prompt by adding specifications for tone and conversational style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add tone and style specifications where you see **********\n",
    "tone_system_prompt = f\"\"\"{persona_system_prompt}\n",
    "\n",
    "Tone and style:\n",
    "- Funny\n",
    "- Kids Audience\n",
    "- simple concepts\n",
    "\n",
    "Answer as if you are Einstein speaking in 1950, reflecting on your life and work. Only discuss\n",
    "information that would have been known to you in your lifetime.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"Can you tell me about relativity?\"\n",
    "\n",
    "print(\"Sending prompt with tone and style specifications...\")\n",
    "tone_response = get_completion(tone_system_prompt, user_prompt)\n",
    "print(\"Response received!\\n\")\n",
    "\n",
    "# Display the last two prompts and responses\n",
    "display_responses(\n",
    "    {\n",
    "        \"system_prompt\": persona_system_prompt,\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"response\": persona_response,\n",
    "    },\n",
    "    {\n",
    "        \"system_prompt\": tone_system_prompt,\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"response\": tone_response,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Q&A Session Format (2 min)\n",
    "\n",
    "Now, let's incorporate a specific Q&A format to test the persona consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add three questions where you see **********\n",
    "user_prompt = \"\"\"\n",
    "Questions:\n",
    "1. Tell me about light Speed\n",
    "2. Tell me about Solar System\n",
    "3. Tell me about Gravity\n",
    "\"\"\"\n",
    "\n",
    "print(\"Sending prompt with Q&A format...\")\n",
    "qa_response = get_completion(tone_system_prompt, user_prompt)\n",
    "print(\"Response received!\\n\")\n",
    "\n",
    "display_responses(\n",
    "    {\n",
    "        \"system_prompt\": tone_system_prompt,\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"response\": qa_response,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Reflection & Transfer (5 min)\n",
    "\n",
    "Let's reflect on what we've learned from this exercise."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which prompt refinement produced the most authentic historical persona and why?\n",
    "\n",
    "Use this cell to jot down your thoughts:\n",
    "\n",
    "`TODO: Add your response where you see **********`\n",
    "\n",
    "Response: `I think the most authentic Historical persona is the one which is defined closer to the Figure we try to emulate in this case Einstein, as it is going to be easier for the model to find closer words to what even the model use for its training. In this case adopting a Academic style and situating the historical moment after second world war it is easy for the model to, in my opinion, produce the most appropiated answes. Trying to answer as funny persona for a kids audience, make this harder and I believe that model didt get a good outcome there`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this exercise, we explored how role-based prompting can be used to create convincing historical figure personas:\n",
    "\n",
    "1. **Plain Prompt**: We started with a simple request involving no role-playing.\n",
    "2. **Baseline Prompt**: We then used a simple request for the AI to role-play as Albert Einstein.\n",
    "3. **Persona-Specific Attributes**: We added details about personality traits, speech style, expertise, and historical context.\n",
    "4. **Tone and Style**: We further refined the prompt with specific instructions about conversational tone and linguistic style.\n",
    "5. **Q&A Format**: We tested the persona with specific questions to evaluate consistency and authenticity.\n",
    "\n",
    "Great work on making it this far! 🎉🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "23393d2575091a37cff0d0e9e7479591a295495b26c3b2ebf9b64da572e02d85"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
