{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 1: Role-Based Prompting (Agent Personas)\n",
    "\n",
    "## Historical Figure Interviewer\n",
    "\n",
    "In this hands-on exercise, you will craft prompts to instruct an AI to convincingly adopt the persona of a historical figure during an interactive Q&A session.\n",
    "\n",
    "## **Outline:**\n",
    "- Plain Prompt: Try a basic prompt without any specific role defined\n",
    "- Baseline Historical Figure Prompt: Ask the AI to role-play as a historical figure\n",
    "- Define Persona-Specific Attributes: Define personality, etc.\n",
    "- Add Tone and Style Specifications: Specify tone and style for the responses\n",
    "- Q&A Session Format: Conduct mock Q&A sessions to test persona consistency\n",
    "- Reflection & Transfer: Evaluate realism of AI responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using the Vocareum API endpoint\n",
    "# TODO: Fill in the missing parts marked with **********\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openai.vocareum.com/v1\",\n",
    "    # Uncomment one of the following\n",
    "    # api_key=\"**********\",  # <--- TODO: Fill in your Vocareum API key here\n",
    "    # api_key=os.getenv(\n",
    "    #     \"OPENAI_API_KEY\"\n",
    "    # ),  # <-- Alternately, set as an environment variable\n",
    ")\n",
    "\n",
    "# If using OpenAI's API endpoint\n",
    "# client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to display responses as Markdown, horizontally\n",
    "def display_responses(*args):\n",
    "    markdown_string = \"<table><tr>\"\n",
    "    # Headers\n",
    "    for arg in args:\n",
    "        markdown_string += f\"<th>System Prompt:<br />{arg['system_prompt']}<br /><br />\"\n",
    "        markdown_string += f\"User Prompt:<br />{arg['user_prompt']}</th>\"\n",
    "    markdown_string += \"</tr>\"\n",
    "    # Rows\n",
    "    markdown_string += \"<tr>\"\n",
    "    for arg in args:\n",
    "        markdown_string += f\"<td>Response:<br />{arg['response']}</td>\"\n",
    "    markdown_string += \"</tr></table>\"\n",
    "    display(Markdown(markdown_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to interact with the OpenAI API\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class OpenAIModels(str, Enum):\n",
    "    GPT_4O_MINI = \"gpt-4o-mini\"\n",
    "    GPT_41_MINI = \"gpt-4.1-mini\"\n",
    "    GPT_41_NANO = \"gpt-4.1-nano\"\n",
    "\n",
    "\n",
    "MODEL = OpenAIModels.GPT_41_NANO\n",
    "\n",
    "\n",
    "def get_completion(system_prompt, user_prompt, model=MODEL):\n",
    "    \"\"\"\n",
    "    Function to get a completion from the OpenAI API.\n",
    "    Args:\n",
    "        system_prompt: The system prompt\n",
    "        user_prompt: The user prompt\n",
    "        model: The model to use (default is gpt-4.1-mini)\n",
    "    Returns:\n",
    "        The completion text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Plain Prompt\n",
    "\n",
    "First, let's see what the model produces with a basic prompt before asking it to role-play as a historical figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_system_prompt = \"You are a helpful assistant.\"\n",
    "user_prompt = \"Can you tell me about relativity?\"\n",
    "\n",
    "print(f\"Sending prompt to {MODEL} model...\")\n",
    "control_response = get_completion(control_system_prompt, user_prompt)\n",
    "print(\"Response received!\\n\")\n",
    "\n",
    "display_responses(\n",
    "    {\n",
    "        \"system_prompt\": control_system_prompt,\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"response\": control_response,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline Historical Figure Prompt\n",
    "\n",
    "First, let's see what the model produces with a basic prompt asking it to role-play as Albert Einstein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add a role-based prompt for portraying Albert Einstein where you see the **********\n",
    "baseline_system_prompt = (\n",
    "    \"Pretend you are Albert Einstein and answer my questions about your work and life.\"\n",
    ")\n",
    "user_prompt = \"Can you tell me about relativity?\"\n",
    "\n",
    "print(f\"Sending prompt to {MODEL} model...\")\n",
    "baseline_response = get_completion(baseline_system_prompt, user_prompt)\n",
    "print(\"Response received!\\n\")\n",
    "\n",
    "display_responses(\n",
    "    {\n",
    "        \"system_prompt\": baseline_system_prompt,\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"response\": baseline_response,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Persona-Specific Attributes (3 min)\n",
    "\n",
    "Now, let's enhance the prompt by adding specific attributes about Einstein's personality, era-specific vocabulary, and expertise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add persona-specific attributes where you see **********\n",
    "persona_system_prompt = f\"\"\"{baseline_system_prompt}.\n",
    "\n",
    "Adopt these persona characteristics:\n",
    "\n",
    "- Personality: Curious, humble yet confident, slightly absentminded, with a playful\n",
    "  sense of humor\n",
    "- Speech style: German-accented English with occasional German phrases, uses\n",
    "  metaphors and thought experiments to explain complex ideas\n",
    "- Expertise: Revolutionary physics theories (relativity, photoelectric effect,\n",
    "  mass-energy equivalence), philosophy of science, and pacifism\n",
    "- Historical context: You lived 1879-1955, worked at the Swiss Patent Office early\n",
    "  in your career, later taught at Princeton, left Germany when Hitler rose to power\n",
    "  \n",
    "Answer as if you are Einstein speaking in 1950, reflecting on your life and work.\n",
    "Only discuss information that would have been known to you in your lifetime.\"\"\"\n",
    "\n",
    "user_prompt = \"Can you tell me about relativity?\"\n",
    "\n",
    "print(f\"Sending prompt to {MODEL} model...\")\n",
    "persona_response = get_completion(persona_system_prompt, user_prompt)\n",
    "print(\"Response received!\\n\")\n",
    "\n",
    "# Show last two prompts and responses\n",
    "display_responses(\n",
    "    {\n",
    "        \"system_prompt\": baseline_system_prompt,\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"response\": baseline_response,\n",
    "    },\n",
    "    {\n",
    "        \"system_prompt\": persona_system_prompt,\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"response\": persona_response,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Add Tone and Style Specifications (3 min)\n",
    "\n",
    "Let's further refine the prompt by adding specifications for tone and conversational style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add tone and style specifications where you see **********\n",
    "tone_system_prompt = f\"\"\"{persona_system_prompt}\n",
    "\n",
    "Tone and style:\n",
    "- Speak in a warm, grandfatherly manner with occasional philosophical tangents\n",
    "- Use \"you see\" and \"imagine, if you will\" when explaining concepts\n",
    "- Express wonder at the universe's mysteries\n",
    "- Show humility about your achievements while being passionate about scientific inquiry\n",
    "- Occasionally make self-deprecating jokes about your hair or poor memory for practical matters\n",
    "\n",
    "Answer as if you are Einstein speaking in 1950, reflecting on your life and work. Only discuss\n",
    "information that would have been known to you in your lifetime.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"Can you tell me about relativity?\"\n",
    "\n",
    "print(\"Sending prompt with tone and style specifications...\")\n",
    "tone_response = get_completion(tone_system_prompt, user_prompt)\n",
    "print(\"Response received!\\n\")\n",
    "\n",
    "# Display the last two prompts and responses\n",
    "display_responses(\n",
    "    {\n",
    "        \"system_prompt\": persona_system_prompt,\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"response\": persona_response,\n",
    "    },\n",
    "    {\n",
    "        \"system_prompt\": tone_system_prompt,\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"response\": tone_response,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Q&A Session Format (2 min)\n",
    "\n",
    "Now, let's incorporate a specific Q&A format to test the persona consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add three questions where you see **********\n",
    "user_prompt = \"\"\"\n",
    "Questions:\n",
    "1. What inspired your theory of relativity?\n",
    "2. How do you feel about the development of atomic weapons?\n",
    "3. What advice would you give to young scientists today?\"\"\"\n",
    "\n",
    "print(\"Sending prompt with Q&A format...\")\n",
    "qa_response = get_completion(tone_system_prompt, user_prompt)\n",
    "print(\"Response received!\\n\")\n",
    "\n",
    "display_responses(\n",
    "    {\n",
    "        \"system_prompt\": tone_system_prompt,\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"response\": qa_response,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Reflection & Transfer (5 min)\n",
    "\n",
    "Let's reflect on what we've learned from this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which prompt refinement produced the most authentic historical persona and why?\n",
    "\n",
    "Use this cell to jot down your thoughts:\n",
    "\n",
    "`TODO: Add your response where you see **********`\n",
    "\n",
    "It's hard to say without using an actual dataset of historical figures' responses. The descriptions provided here of Einstein's persona's characteristics may not be the best way to represent him. On first read, however, the model seems to have followed the provided instructions relatively well.\n",
    "\n",
    "With a large dataset of the historical figure's spoken and written words patterns, we could feasibly fine tune a model to more closely match these patterns. We could then analyze the statistical properties of the ground truth data and the generated data to compare things such as average sentence length and word frequency to see which prompt refinement was more authentic. We would need to generate a lot more text, though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this exercise, we explored how role-based prompting can be used to create convincing historical figure personas:\n",
    "\n",
    "1. **Plain Prompt**: We started with a simple request involving no role-playing.\n",
    "2. **Baseline Prompt**: We then used a simple request for the AI to role-play as Albert Einstein.\n",
    "3. **Persona-Specific Attributes**: We added details about personality traits, speech style, expertise, and historical context.\n",
    "4. **Tone and Style**: We further refined the prompt with specific instructions about conversational tone and linguistic style.\n",
    "5. **Q&A Format**: We tested the persona with specific questions to evaluate consistency and authenticity.\n",
    "\n",
    "Great work on making it this far! 🎉🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0 (main, Dec  3 2024, 02:24:14) [GCC 10.2.1 20210110]"
  },
  "vscode": {
   "interpreter": {
    "hash": "23393d2575091a37cff0d0e9e7479591a295495b26c3b2ebf9b64da572e02d85"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
